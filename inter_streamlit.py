from gensim.models import Word2Vec
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re 
from collections import defaultdict
import pandas as pd
import folium
from streamlit_folium import folium_static
from folium import plugins
from googletrans import Translator
from PIL import Image
import seaborn as sns
import plotly.express as px
from meta import * # file with long texts
#spreadsheet check
# from gsheetsdb import connect
# from gspread_pandas import Spread,Client
# from google.oauth2 import service_account



st.set_page_config(layout="wide")

#Create a Google Authentication connection object
# scope = ['https://spreadsheets.google.com/feeds',
#          'https://www.googleapis.com/auth/drive']

# credentials = service_account.Credentials.from_service_account_info(
#                 st.secrets["gcp_service_account"], scopes = scope)
# client = Client(scope=scope,creds=credentials)
# spreadsheetname = "Input_holder"
# spread = Spread(spreadsheetname,client = client)


#mean vectorizer
class MeanEmbeddingVectorizer(object):
    def __init__(self, model_cbow):
        self.model_cbow = model_cbow
        self.vector_size = model_cbow.wv.vector_size

    def fit(self):  
        return self

    def transform(self, docs): 
        doc_word_vector = self.doc_average_list(docs)
        return doc_word_vector

    def doc_average(self, doc):
        mean = []
        for word in doc:
            if word in self.model_cbow.wv.index_to_key:
                mean.append(self.model_cbow.wv.get_vector(word))

        if not mean: 
            return np.zeros(self.vector_size)
        else:
            mean = np.array(mean).mean(axis=0)
            return mean

    def doc_average_list(self, docs):
        return np.vstack([self.doc_average(doc) for doc in docs])

#tf-idf vectorized
class TfidfEmbeddingVectorizer(object):
    def __init__(self, model_cbow):

        self.model_cbow = model_cbow
        self.word_idf_weight = None
        self.vector_size = model_cbow.wv.vector_size

    def fit(self, docs): 


        text_docs = []
        for doc in docs:
            text_docs.append(" ".join(doc))

        tfidf = TfidfVectorizer()
        tfidf.fit(text_docs)  
        # if a word was never seen it is given idf of the max of known idf value
        max_idf = max(tfidf.idf_)  
        self.word_idf_weight = defaultdict(
            lambda: max_idf,
            [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()],
        )
        return self

    def transform(self, docs): 
        doc_word_vector = self.doc_average_list(docs)
        return doc_word_vector

    def doc_average(self, doc):


        mean = []
        for word in doc:
            if word in self.model_cbow.wv.index_to_key:
                mean.append(
                    self.model_cbow.wv.get_vector(word) * self.word_idf_weight[word]
                ) 

        if not mean:  
            return np.zeros(self.vector_size)
        else:
            mean = np.array(mean).mean(axis=0)
            return mean
    def doc_average_list(self, docs):
      return np.vstack([self.doc_average(doc) for doc in docs])

@st.cache(allow_output_mutation=True)
def load_data(check): 
    if check: 
        data = pd.read_excel('main_data-2-2_copy (1).xlsx', index_col=0)
        embeddings = pd.read_pickle('embed.pickle')
        clean_words = pd.read_pickle('words.pickle')
        swords = pd.read_pickle('swords.pickle')
        latlong = pd.read_csv('LATandLONG.csv', index_col=0)
        progs = pd.read_pickle('nwconstr.pickle')
    return data, embeddings, clean_words, swords, latlong, progs
data, doc_vec, clean_words, swords, latlong, progs = load_data(True)
data = data[data['tuition_EUR']<90000] #looks shitty, but i don't have ehough time... haha)

# @st.cache(allow_output_mutation=True)
# def corpus_l(data):
#     return list(data)

@st.cache(allow_output_mutation=True)
def load_model(mpath): 
    return Word2Vec.load(mpath)

#load up some cleaning functions
def tokenization(text):
    tokens = re.split('\s+',text)
    return tokens

def remove_stopwords(text):
    output= [i for i in text if i not in swords]
    return output

def len_control(text):
  lemm_text = [word for word in text if len(word)>=3]
  return lemm_text

def sorter(text):
  sorted_list = sorted(text)
  return sorted_list

def make_clickable(name, link):
    # target _blank to open new window
    # extract clickable text to display for your link
    text = name
    return f'<a target="_blank" href="{link}">{text}</a>'

def program_parser2(data):
    for i in range(data.shape[0]):
        data.Introduction[i] = str(re.sub('[0-9]+',' ',re.sub(r'[^\w\s]',' ',re.sub('\\\\n', ' ' ,re.sub('&.*?;.*?;|&.*?;|._....',' ',str(data.Introduction[i]))))).lower().strip())
    data['msg_sorted_clean']= (data['Introduction']
                               .apply(lambda x: tokenization(x))
                               .apply(lambda x:remove_stopwords(x))
                               .apply(lambda x:len_control(x))
                               .apply(lambda x: sorter(x)))
    return data

def pick_n_pretty(df):
    output = df[['Link', 'program', 'university', 'country', 'city', 'language', 'tuition_EUR','Score']]
    output["Link"] = output.apply(
            lambda row: make_clickable(row["program"], row["Link"]), axis=1)
    output['tuition_EUR'] = output['tuition_EUR'].fillna(0)
    output['tuition_EUR'] = output.apply(lambda row: int(row['tuition_EUR']), axis=1)
    return output#.style.applymap(lambda x: "background-color: red" if x==0 else "background-color: white")


def get_recommendations(N, scores, data_path = 'main_data-2-2_copy (1).xlsx'):
    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]
    data = (pd.read_excel(data_path, index_col = 0)
           .drop(columns = ['msg_sorted_clean'])
           .loc[top]
           .reset_index())
    data['Score'] = sorted(scores, reverse=True)[:N]
    return data

def get_recs(sentence, N=10, mean=False):
    '''Get top-N recommendations based on your input'''
    input = pd.DataFrame({'Introduction': [str(sentence)]})
    input = program_parser2(input)
    input_embedding = tfidf_vec_tr.transform([input['msg_sorted_clean'][0]])[0].reshape(1, -1)
    cos_sim = map(lambda x: cosine_similarity(input_embedding, x)[0][0], doc_vec)
    scores = list(cos_sim)
    recommendations = get_recommendations(N,scores)
    return recommendations

def mfap(recs1, df=latlong):
    latlong = recs1.merge(df, left_on='city', right_on='location', how = 'inner')      
    uni_locations = latlong[["lat", "long", "location"]]
    map = folium.Map(location=[uni_locations.lat.mean(), uni_locations.long.mean()], zoom_start=4, control_scale=True)
    for index, location_info in uni_locations.iterrows():
        folium.Marker([location_info["lat"], location_info["long"]], popup=location_info["location"]).add_to(map)
    return map

def mfap_density_50(recs50, df=latlong): #try this function on the main page
    latlong = recs50.merge(df, left_on='city', right_on='location', how = 'inner')
    uni_locations = latlong[["lat", "long"]]
    map = folium.Map(location=[uni_locations.lat.mean(), uni_locations.long.mean()], zoom_start=4, control_scale=True)
    cityArr = uni_locations.values
    map.add_child(plugins.HeatMap(cityArr, radius=25))
    return map

def sim_prog(df=progs, prog=None):
    df_one = df[df['Program1']==prog]
    return df_one.sort_values(by='cosine', ascending=False)
#yes.... the code is suboptimal, but i don't care about image.pngs point now ;)
def p2p_locs(latlong=latlong, uni_info=data, recs=[], N=5): #recs is the output of sim_progs #density map for similar universities
    recs[['Uni', 'Prog']] = recs['Program2'].str.split(': ', 1, expand=True)
    recs[['Uni1', 'Prog1']] = recs['Program1'].str.split(': ', 1, expand=True)
    ps = (recs
            .merge(uni_info, left_on=['Uni', 'Prog'], right_on=['university','program'], how = 'inner')
            .merge(latlong, left_on='city', right_on='location', how ='inner'))
    fin_rec = ps[['Program1', 'Program2', 'city','cosine']].reset_index().iloc[1:N+1,:]
    uni_locations = ps[["lat", "long"]]
    map = folium.Map(location=[uni_locations.lat.mean(), uni_locations.long.mean()], zoom_start=4, control_scale=True)
    cityArr = uni_locations.values
    map.add_child(plugins.HeatMap(cityArr, radius=25))
    return map, fin_rec, ps 

def simple_output(map=True):
    col1, col2, col3 = st.columns([10, 10, 10])
    with col2:
        gif_runner = st.image("200.gif")
    recs1 = get_recs(str(text), N=int(number), mean=False)
    recs50 = get_recs(str(text), N=50, mean=False)
    recs1 = pick_n_pretty(recs1)
    gif_runner.empty()
    df = recs1.style.background_gradient(
        cmap=cmGreen,
        subset=[
            "Score",
        ],
    )
    st.write(df.to_html(escape=False), unsafe_allow_html=True)  
    if map:
        map2 = mfap_density_50(recs50) 
        map  = mfap(recs1)
        st.write('')
        st.write('')
        with st.expander('–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –∫–∞—Ä—Ç—ã üåç'):
            ce, c1, ce, c2, c3 = st.columns([0.07, 4, 0.07, 4, 0.07])
            with c1:
                st.write('–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤')
                folium_static(map, width=450) 
            with c2:
                st.write('POI-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–æ–≤ —Ç–æ–ø-50 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –í–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –ø—Ä–æ–≥—Ä–∞–º–º')
                folium_static(map2, width=450)


with st.sidebar:
    col1, col2, col3 =st.columns([2.2, 6, 2.2])
    with col1:
        st.write("")
    with col2:
        st.image('keystone-masters-degree.jpg') 
    with col3:
        st.write('')
    page = st.radio('–°—Ç—Ä–∞–Ω–∏—Ü–∞', ['–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µüëã',"–ù–∞–π—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º—Éüåç", "–ù–∞–π—Ç–∏ —Å—Ö–æ–∂–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ãüôå","–î–∞–Ω–Ω—ã–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞üìà"])
    
    # st.subheader('–í—ã–±–µ—Ä–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã')
    # location = st.multiselect('–°—Ç—Ä–∞–Ω–∞', list(set(data['country'])))
    # on_site = st.selectbox('–¢–µ–º–ø –æ–±—É—á–µ–Ω–∏—è', ['–û—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–ó–∞–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ','–û—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ|–ó–∞–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'])
    # pace = st.selectbox('–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è', ['–û–Ω–ª–∞–π–Ω', '–ö–∞–º–ø—É—Å','–ö–∞–º–ø—É—Å|–û–Ω–ª–∞–π–Ω'])
    # lang = st.selectbox('–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è', list(set(data['Language'].dropna())))
    # cost = st.slider('–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, EUR', int(data['tuition_EUR'].min()), int(data['tuition_EUR'].max()), (0, 3000), step=50)

# Page 1-Intro
if page=='–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µüëã':
    #_max_width_()
    c30, c31, c32 = st.columns([2.5, 1, 3])

    with c30:
        st.image("keystone-masters-degree.jpg", width=400)
    

    st.markdown("""
            –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å, –¥–æ—Ä–æ–≥–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å!
            
            –†–µ—à–∏–ª –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –Ω–∞ –±–∞–∫–∞–ª–∞–≤—Ä–∏–∞—Ç–µ –∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –≥—Ä—ã–∑—Ç—å –≥—Ä–∞–Ω–∏—Ç –Ω–∞—É–∫–∏ –≤ –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–µ? –ó–Ω–∞—á–∏—Ç, —Ç—ã –ø–æ –≤–µ—Ä–Ω–æ–º—É –∞–¥—Ä–µ—Å—É!

            –ú—ã –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º —Ç–µ–±–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∏ –∑–∞—Ä—É–±–µ–∂–Ω—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–≤–æ–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π!

            –°–∞–π—Ç –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è —Ç–µ—Ö, –∫—Ç–æ –∂–µ–ª–∞–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ –ø–æ–∏—Å–∫–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã, —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –∏–ª–∏ —Å—Ç—Ä–∞–Ω—ã.
        """, unsafe_allow_html = True)

    with st.expander("‚ÑπÔ∏è - –ò–¥–µ—è –ø—Ä–æ–µ–∫—Ç–∞", expanded=False):

        st.markdown(
            """ 
–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ —Å–≤–æ–∏—Ö —Ä–æ–¥–Ω—ã—Ö –≥–æ—Ä–æ–¥–∞—Ö, –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –∏–∑ –°–ù–ì, –∂–µ–ª–∞—é—â–∏—Ö –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª—å—à–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π '–∫–æ—Ä–æ—á–∫–∏' –æ –∫–∞–∫–æ–π-–ª–∏–±–æ —Å—Ç–µ–ø–µ–Ω–∏. –í—ã–±–æ—Ä —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Å–≤–æ–µ–≥–æ –≥–æ—Ä–æ–¥–∞, –∏ –¥–∞–∂–µ —Å—Ç—Ä–∞–Ω—ã, –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–º–∏ —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏, —Å—Ç–æ–∏–º–æ—Å—Ç—å—é –∏ –¥—Ä—É–≥–∏–º–∏ –∏–∑–º–µ—Ä–∏–º—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏. *–ü–æ —ç—Ç–æ–π –ø—Ä–∏—á–∏–Ω–µ, –Ω–∞—à–∞ –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –≥—Ä—É–ø–ø–∞ –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–ª–∞—Å—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞–π—Ç–∏ —á—Ç–æ-—Ç–æ –±–ª–∏–∑–∫–æ–µ –∫ —Å–≤–æ–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –Ω–µ —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–º, –Ω–æ –∏ –º–µ–Ω–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–º –≤ —Å–µ–±–µ —Å–ª—É—à–∞—Ç–µ–ª—è–º.* –ù–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–µ:

1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ –æ–∑–Ω–∞–∫–∞–º–ª–∏–≤–∞–µ—Ç—Å—è —Å –±–ª–∏–∑–∫–∏–º–∏ –∫ –µ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—é —Å–≤–æ–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π, –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –∏ –∂–µ–ª–∞–Ω–∏–π –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—á–µ–≤–∏–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ä–æ–≤, –ª–æ–∫–∞—Ü–∏–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —Å–∞–π—Ç–∞ Keystone (–Ω–∞ —Å—Ä–∞–Ω–∏—Ü—ã –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–æ–æ—Ç–≤–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Å—ã–ª–∫–∏).

2. –ú–æ–∂–µ—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å–≤–æ–π —Å–ø–∏—Å–æ–∫ –ø–æ—Ç–µ–Ω—Ü–∏–ª—å–Ω—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –≤—ã–±—Ä–∞–≤ —Å–∞–º—É—é –ª—É—á—à—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º.

3. –ú–æ–∂–µ—Ç –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –Ω–∞—à–µ–π –±–∞–∑–æ–π, –∞ –¥–∞–ª—å—à–µ –∏–∑—É—á–∏—Ç—å –æ–± —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –∏–∑ –±–∞–∑—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤.  

*–ù–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –±—É–¥–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è! –ù–∞—à–∞ —Ç–µ–±–µ –¥–∞–∂–µ –µ—Å–ª–∏ —Ç—ã —Å–æ–≤—Å–µ–º –Ω–µ –∑–Ω–∞–µ—à—å —á–µ–≥–æ —Ç—ã —Ö–æ—á–µ—à—å!* 
            """
        )

        st.markdown("")

    col1, col2 = st.columns([5,5])
    with col1:
        with st.expander("–î–∞–Ω–Ω—ã–µ", expanded=False):

            st.markdown( """
            –ü—Ä–æ–µ–∫—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–∞ [masterstudies.ru](https://masterstudies.com). –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ–∫–∞–∑–∞–ª—Å—è –±–æ–ª–µ–µ –∫–æ–º–ª–µ–∫—Å–Ω–æ–π –∑–∞–¥–∞—á–µ–π, —á–µ–º –æ–∂–∏–¥–∞–ª–æ—Å—å –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ. –ü—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ —Å–æ—Å—Ç–æ—è–ª –∏–∑ —Ç—Ä—ë—Ö —ç—Ç–∞–ø–æ–≤:

            *   –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã —Å–∞–π—Ç–∞ —Å –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π ('–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è','–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å–æ–º', '–ï—Å–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—É–∫–∏' –∏ –¥—Ä—É–≥–∏–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –≤ –ù–ò–£ –í–®–≠ (–°–ü–±))
            *   –°–±–æ—Ä –æ–±—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö () –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫ –ø—Ä–æ–≥—Ä–∞–º–º (–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã, –Ω–∞–∑–≤–∞–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞, –≤–∏–¥ –ø—Ä–æ–≥—Ä–∞–º–º—ã, –Ω–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã, —Ñ–æ—Ä–º–∞—Ç –∏ —Ç–∏–ø –æ–±—É—á–µ–Ω–∏—è, —Å—Ç—Ä–∞–Ω–∞, —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, —è–∑—ã–∫ –æ–±—É—á–µ–Ω–∏—è, –∏ –¥–µ–¥–ª–∞–π–Ω—ã –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–æ–∫)
            * –°–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ–≥—Ä–∞–º–º –Ω–∞ Keystone 

            –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ø—Ä–æ—Ö–æ–¥—è—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä—É –æ—á–∏—Å—Ç–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–ª—É—á–∏–ª–∞ –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –Ω–∞–¥ —Ñ–∏—á–∞–º–∏. 
            * –ú—ã –æ—á–∏—Å—Ç–∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –æ—Ç –≤—ã–±—Ä–æ—Å–æ–≤, –ø—Ä–∏–≤–µ–ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
            * –ú—ã —Å–æ–∑–¥–∞–ª–∏ –¥–≤–∞ –Ω–æ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞: –æ–¥–∏–Ω —Å –ª–æ–∫–∞—Ü–∏—è–º–∏ –∏ –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –≥–æ—Ä–æ–¥–æ–≤, –∞ –¥—Ä—É–≥–æ–π —Å –±–ª–∏–∑–æ—Å—Ç—å—é –ø—Ä–æ–≥—Ä–∞–º–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –ø—Ä–æ–≥—Ä–∞–º–º

            –í –∏—Ç–æ–≥–µ –±—ã–ª –ø–æ–ª—É—á–µ–Ω –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —á–∏—Å—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å 6502 –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.
    """, unsafe_allow_html = True)

            st.markdown("")
    with col2:
        with st.expander("M–µ—Ç–æ–¥—ã ", expanded=False):

            st.write(
                """

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞**

–î–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –Ω–∞–º –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ *–∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ* —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏–π –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö. –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª–∞ –≤—ã–±—Ä–∞–Ω–∞ –≤–∞—Ä–∏–∞—Ü–∏—è CBOW (continuous bag of words) Word2Vec, –¥–æ –∫–æ—Ç–æ—Ä–æ–π –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ—à–ª–∏ –Ω–µ–±–æ–ª—å—à—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É - –æ—á–∏—Å—Ç–∫–∞ –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ü–∏—Ñ—Ä, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è –±–æ–ª–µ–µ –∫–æ—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã Word2Vec. –î–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –≤ —Å–∏–ª—É –ø—Ä–∏–Ω—Ü–∏–ø–∞ —Ä–∞–±–æ—Ç—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–∫—Ä—É–∂–∞—é—â–∏—Ö –µ–≥–æ —Å–ª–æ–≤. –î–∞–ª–µ–µ –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (–æ–ø–∏—Å–∞–Ω–∏—è –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –≤–≤–æ–¥—ã) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –≤–∏–¥–µ –ø—Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ IDF, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–∑–Ω–∞—á–∞–µ—Ç –º–µ–Ω—å—à–∏–π –≤–µ—Å –±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞–º -- —ç—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –¥–æ—Å—Ç–∏—á—å –±–æ–ª—å—à–µ–π —Ä–∞–∑–ª–∏—á–∏—Ç–µ–ª—å–Ω–æ–π —Å–∏–ª—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞. 

 * –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –≤–≤–æ–¥—ã

–ú—ã –Ω–∞—Ü–µ–ª–µ–Ω—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–≥–æ–≤–æ—Ä—è—â–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø–æ—ç—Ç–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –≤–≤–æ–¥—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ü–æ —ç—Ç–æ–π –ø—Ä–∏—á–∏–Ω–µ, –º—ã –≤–Ω–µ–¥—Ä–∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —É–∂–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–¥—Å—á–µ—Ç–∞ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –æ–ø–∏—Å–∞–Ω–∏–π –ø—Ä–æ–≥—Ä–∞–º–º –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –≤–≤–æ–¥–æ–º.

 * –ü–æ–ø–∞—Ä–Ω–æ–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º

–î–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏ –º—ã –ø–æ—Å—á–∏—Ç–∞–ª–∏ –Ω—É–∂–Ω—ã–º –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥—Å—á–µ—Ç–∞ –ø–æ–ø–∞—Ä–Ω—ã—Ö –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏ –≤ –±–∞–∑–µ. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –æ–ø—Ü–∏–∏ —Å–æ —Å—Ö–æ–∂–µ—Å—Ç—å—é –±–æ–ª–µ–µ 0,65 (–∏–Ω–∞—á–µ –±—ã–ª–æ –±—ã —Å–ª–æ–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –±–∞–∑–æ–π –∏–∑ >40 –º–ª–Ω –ø–∞—Ä. 

 * –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã –ª–æ–∫–∞—Ü–∏–∏ —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º, –ø–æ—ç—Ç–æ–º—É –º—ã –ø–æ—Å—Ç—Ä–æ–∏–ª–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –∫–∞—Ä—Ç—ã POI (–∏–ª–∏ KDE-–ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ –Ω–∞ –∫–∞—Ä—Ç–µ) –∏ –ª–æ–∫–∞—Ü–∏–π –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞.

""")

            st.markdown("")

    st.markdown("")
    #st.write(spread.url)

   # st.markdown(hello, unsafe_allow_html = True)

if page=='–ù–∞–π—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º—Éüåç':
    #_max_width_()
    c30, c31, c32 = st.columns([2.5, 1, 3])

    with c30:
        st.image("keystone-masters-degree.jpg", width=400)

    with st.expander("‚ÑπÔ∏è - –û–± —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ + –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", expanded=False):

        st.markdown("""
                –ù–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ç—ã —Å–º–æ–∂–µ—à—å –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –±–ª–∏–∑–∫–∏ —Ç–≤–æ–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ —Ç–∏–ø–∞–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—Å—Ç –≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏, –ø–æ—ç—Ç–æ–º—É —Ä–∞—Å—Å–∫–∞–∂–µ–º —Ç–æ–ª—å–∫–æ –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –£ —Ç–µ–±—è –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—â–∏—Ö –∏ –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ç–≤–æ–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π.

                * –ï—Å–ª–∏ —Ç—ã –∂–µ–ª–∞–µ—à—å –ø–æ–ª—É—á–∏—Ç—å *–≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã*, —Ç–æ —Ç–µ–±–µ —Å—Ç–æ–∏—Ç: –ü–û–°–¢–ê–í–ò–¢–¨ –ì–ê–õ–û–ß–ö–£ —Å–ª–µ–≤–∞ –æ—Ç '–í—ã–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é', –≤—ã–±—Ä–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥—Ä–∞–º–º, –≤–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –æ —Å–≤–æ–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–∞—Ö, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è—Ö –∏–ª–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è—Ö - —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–π –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ. 

                * –ï—Å–ª–∏ –¥–µ—Ç–∞–ª–∏ –≤—Å–µ-—Ç–∞–∫–∏ –≤–∞–∂–Ω—ã, —Ç–æ —Ç–µ–±–µ —Å—Ç–æ–∏—Ç –∑–∞–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ –ø–æ–ª—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –≤–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç, –Ω–æ –ù–ï –°–¢–ê–í–ò–¢–¨ –ì–ê–õ–û–ß–ö–£

                **–ö—Ä–æ–º–µ —ç—Ç–æ–≥–æ, –≤–∞–∂–Ω–æ –∑–∞–ø–æ–º–Ω–∏—Ç—å –æ–± –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –±—É–¥–µ—Ç —É–≤–∏–¥–µ—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∑–∞–ø—Ä–æ—Å–∞ -- —É —Ç–µ–±—è –±—É–¥–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç—å –∏–ª–∏ —Ä–∞—Å–∫—Ä—ã—Ç—å —Ç–∞–±–ª–∏—Ü—ã –∏ –∫–∞—Ä—Ç—ã.**
                
                –í–ê–ñ–ù–û: –Ø–∑—ã–∫–∏ –∏ —Å—Ç—Ä–∞–Ω—ã –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –æ–±—ã—á–Ω—ã–º –≤–≤–æ–¥–æ–º, —á—Ç–æ–±—ã —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ. 

                –°–ø–∞—Å–∏–±–æ –∑–∞ –∏–Ω—Ç–µ—Ä–µ—Å! –£–¥–∞—á–∏!  
        """)

        st.markdown("")

    st.markdown("")

    #scenario_interact = st.selectbox(
     #   "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –æ–ø—ã—Ç–∞",
      #  ["–•–æ—á—É –Ω–∞–π—Ç–∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–•–æ—á—É —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å–≤–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≥—Ä–∞–º–º"],
    

    with st.form(key="my_form"):

        ce, c1, ce, c2, c3 = st.columns([0.07, 2, 0.07, 4, 0.07])
        with c1:
            st.subheader('–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã') 
            number = st.number_input('–°–∫–æ–ª—å–∫–æ —Ä–µ–∫–æ–º–º–µ–Ω–¥–∞—Ü–∏–π –∂–µ–ª–∞–µ—à—å —É–≤–∏–¥–µ—Ç—å –Ω–∞ —ç–∫—Ä–∞–Ω–µ?', min_value=0, max_value=50, step=1, value=5)
            agree = st.checkbox('–í—ã–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é')
            location = st.multiselect('–°—Ç—Ä–∞–Ω–∞', sorted(list(set(data['country'].dropna()))))
            dur = st.slider('–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (–º–µ—Å)', int(data['duration_month'].min()), int(data['duration_month'].max()), (0, 10), step=2)
            on_site = st.selectbox('–¢–∏–ø –æ–±—É—á–µ–Ω–∏—è', ['–û—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–ó–∞–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ','–û—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ|–ó–∞–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'])
            pace = st.selectbox('–§–æ—Ä–º–∞—Ç –æ–±—É—á–µ–Ω–∏—è', ['–û–Ω–ª–∞–π–Ω', '–ö–∞–º–ø—É—Å','–ö–∞–º–ø—É—Å|–û–Ω–ª–∞–π–Ω'])
            lang = st.multiselect('–Ø–∑—ã–∫ –æ–±—É—á–µ–Ω–∏—è', sorted(list(set(data['language'].dropna()))))
            cost = st.slider('–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –≤ –≥–æ–¥, EUR', int(data['tuition_EUR'].min()), int(data['tuition_EUR'].max()), (0, 8000), step=50)
        with c2:
             #to make row effects
            st.markdown('')
            st.markdown('')
            sentence = st.text_area("–í–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–≤–æ–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π -- –º–æ–∂–µ—à—å –≤–≤–µ—Å—Ç–∏ —á—Ç–æ —É–≥–æ–¥–Ω–æ, –Ω–æ —Ü–∏—Ñ—Ä—ã –∏ —Å–∏–º–≤–æ–ª—ã –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º–æ–π", value='–ù–∞–ø—Ä–∏–º–µ—Ä: —è –∑–Ω–∞—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –ø—Ä–æ—à–µ–ª –∫—É—Ä—Å—ã –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ —Ä—ã–Ω–∫–∞–º–∏')
            submit = st.form_submit_button(label="‚ú® –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é")
            corpus = list(clean_words)
            model = load_model('model_cbow.bin')
            model.init_sims(replace=True)
            tfidf_vec_tr = TfidfEmbeddingVectorizer(model)
            tfidf_vec_tr.fit(corpus)
            translator = Translator()
            result = translator.translate(sentence)
            text = result.text

    if not submit:
        st.stop()

    cmGreen = sns.light_palette("green", as_cmap=True)
    if submit:
        if len(text)==0:
            st.warning('–¢—ã –Ω–µ —Ä–∞—Å—Å–∫–∞–∑–∞–ª(–∞) –æ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö! –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —Å–∏—Å—Ç–µ–º–∞ –≤—ã–¥–∞—Å—Ç –ø–µ—Ä–≤—ã–µ {} —Å—Ç—Ä–æ–∫(–∏) –Ω–∞—à–µ–π –±–∞–∑—ã —Å –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏.... –≠—Ç–æ –Ω–µ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ'.format(number))
            simple_output()
        else:
            if not agree:
                if len(location)>0 and len(lang)>0:  
                    col1, col2, col3 = st.columns([10, 10, 10])
                    with col1:
                        st.write('')
                    with col2:
                        gif_runner = st.image("200.gif")
                    with col3:
                        st.write('')
                    recs = get_recs(str(text), N=int(data.shape[0]), mean=False)
                    recs50 = get_recs(str(text), N=50, mean=False) #–≤–æ—Ç –∑–¥–µ—Å—å –Ω–∞–¥–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä–æ –≤–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∂–µ–ª–∞–µ–º—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º, –∞ –ø–æ—Ç–æ–º –ø—Ä–æ—Å—Ç–æ –≤—ã–¥–∞–≤–∞—Ç—å —Ç–æ–ø 
                    gif_runner.empty()  
                    recs1 = recs[(recs['language'].isin(list(lang))) & (recs['country'].isin(list(location))) & (recs['on_site']==on_site) & (recs['format']==pace)  & (recs['tuition_EUR']>min(cost)) & (recs['tuition_EUR']<max(cost)) & (recs['duration_month']>min(dur)) & (recs['duration_month']<=max(dur))]
                    recs1 = recs1.reset_index().iloc[:number,:]
                    if recs1.shape[0]!=0:
                        recs2 = pick_n_pretty(recs1)
                        df = recs2.style.background_gradient(
                            cmap=cmGreen,
                            subset=[
                                "Score",
                            ],
                        )
                        st.write(df.to_html(escape=False), unsafe_allow_html=True)
                        map2 = mfap_density_50(recs50)
                        map  = mfap(recs2)
                        st.write('')
                        st.write('')
                        with st.expander('–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –∫–∞—Ä—Ç—ãüåç'):
                            ce, c1, ce, c2, c3 = st.columns([0.07, 4, 0.07, 4, 0.07])
                            with c1:
                                st.write('–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤')
                                folium_static(map, width=450) 
                            with c2:
                                st.write('POI-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–æ–≤ —Ç–æ–ø-50 –ø—Ä–æ–≥—Ä–∞–º–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–≤–æ–µ–º—É –∑–∞–ø—Ä–æ—Å—É')
                                folium_static(map2, width=450)
                        if recs1.shape[0]<number:
                            st.warning("–£–ø—Å... –ü—Ä–æ–≥—Ä–∞–º–º –º–µ–Ω—å—à–µ —á–µ–º –æ–∂–∏–¥–∞–ª–æ—Å—å, –Ω–æ —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å... –û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –æ–ø—Ü–∏—é –Ω–∏–∂–µ)")
                            recs1 = recs.copy()
                            recs2 = pick_n_pretty(recs1)
                            map3 = mfap(recs2)
                            df = recs2.style.background_gradient(
                                cmap=cmGreen,
                                subset=[
                                    "Score",
                                ],
                            )
                            with st.expander('–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–ø—Ü–∏—è–º–∏ –∏–∑ –Ω–∞—à–µ–π –±–∞–∑—ã üëâ'):
                                st.write(df.to_html(escape=False), unsafe_allow_html=True)
                                C, D, E = st.columns([2,5,2])
                                with C:
                                    st.write('')
                                with D:
                                    st.write('–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –≤—ã—à–µ —Ç–∞–±–ª–∏—Ü—ã')
                                    folium_static(map3)
                                with E:
                                    st.write('')

                            
                    else:
                        st.warning('–ú—ã –Ω–µ —Å–º–æ–≥–ª–∏ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–≤–æ–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º, –Ω–æ –ø—Ä–æ—Å–∏–º –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –≤ –Ω–∞—à–µ–π –±–∞–∑–µ ')
                        simple_output()
                else: 
                    st.warning('–ú—ã –Ω–µ —Å–º–æ–≥–ª–∏ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–≤–æ–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º, –Ω–æ –ø—Ä–æ—Å–∏–º –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –≤ –Ω–∞—à–µ–π –±–∞–∑–µ ')
                    simple_output()
                    # st.write('This is an error') #–ù–∞–¥–æ –±—É–¥–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–æ–ø–∏—Å–∞—Ç—å

            else: 
                simple_output()
if page=='–ù–∞–π—Ç–∏ —Å—Ö–æ–∂–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ãüôå':
    c30, c31, c32 = st.columns([2.5, 1, 3])

    with c30:
        st.image("keystone-masters-degree.jpg", width=400)

    with st.expander("‚ÑπÔ∏è - –û–± —ç—Ç–æ–π —Å—Ç–∞—Ä–Ω–∏—Ü–µ + –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è", expanded=False):

        st.write("–ù–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ç—ã —Å–º–æ–∂–µ—à—å –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏, –ø–æ—Ö–æ–∂–∏–º–∏ –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω—É—é —Ç–æ–±–æ–π. –¢—ã –º–æ–∂–µ—à—å –≤—ã–±—Ä–∞—Ç—å –∏–ª–∏ –≤–≤–µ—Å—Ç–∏ –ø–µ—Ä–≤—ã–µ –±—É–∫–≤—ã –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã, —á—Ç–æ–±—ã –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –∞–≤—Ç–æ–∑–∞–≤–ø–æ–ª–Ω–µ–Ω–∏–µ.")

        st.markdown("")

    st.markdown("")    

    st.write('–í—ã–±–µ—Ä–∏ –æ–¥–Ω—É –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ö–æ–∂–∏—Ö')
    with st.form(key="my_form"):
        university_pick = st.selectbox("–°–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ –Ω–∞—à–µ–π –±–∞–∑–µ –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º", list(set(progs['Program1'].dropna())))
        number_sim = st.number_input('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º', min_value=0, max_value=50, step=1, value=5)
        submit = st.form_submit_button(label="‚ú® –ü–æ–∫–∞–∑–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã")
        cmGreen = sns.light_palette("green", as_cmap=True)
    if submit:
        recs = sim_prog(progs, str(university_pick))
        map, recs0, ps = p2p_locs(recs=recs, N=number_sim) #ps is a dirty dataset
        df = recs0.sort_values(by='cosine', ascending=False).style.background_gradient(
            cmap=cmGreen,
            subset=[
                "cosine",
            ],
        )
        see_data = st.expander('–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ö–æ–∂–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã üëâ')
        with see_data:
            st.write(df.to_html(escape=False), unsafe_allow_html=True)
        st.write('')
        ce, c1, ce, c2, c3 = st.columns([0.07, 4, 0.07, 4, 0.07])
        with c1:
            st.write('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º')
            folium_static(map, width=500)
        with c2:
            st.write('')
            c_metric = ps.groupby('Program1')['Program2'].count()[0]
            st.metric(label='–°—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º', value='{}'.format(c_metric))
            d_metric = ps[ps.Uni==ps.Uni1].shape[0]
            st.metric(label='–í –æ–¥–Ω–æ–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ', value='{}'.format(d_metric))
            top = ps.groupby(['city'])['Program2'].agg(['count']).sort_values(by = 'count', ascending=False)
            df = pd.DataFrame({'–ì–æ—Ä–æ–¥':list(top.index), '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': list(top['count'])})
            with st.expander('–í—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç—å —Å—Ç—Ä–∞–Ω —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º', expanded=True):
                st.dataframe(data=df)
            st.text('')



   
if page == '–î–∞–Ω–Ω—ã–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞üìà':
    #_max_width_()
    #data = pd.read_excel('main_data-2.xlsx', index_col=0)
    c30, c31, c32 = st.columns([2.5, 1, 3])

    with c30:
        st.image("keystone-masters-degree.jpg", width=400)


        st.markdown("")
    see_data = st.expander('–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–∞–Ω–Ω—ã–µ üëâ')
    with see_data:
        data['duration_month'] = data['duration_month'].astype('str')
        st.dataframe(data=data)
    st.text('')

    with st.expander('–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞'):

        col1, col2, col3, col4, col5, col6, col7 = st.columns([1,4,1,4,1,4,1])
        qprogs, qcountry, quni = data.shape[0], len(set(data['country'])), len(set(data['university'])) 
        col1.write('')
        col2.metric("–°—Ç—Ä–∞–Ω üåê", "{}".format(qcountry))
        col3.write('')
        col4.metric("–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ üéì","1440") #"{}".format(quni))
        col5.write('')
        col6.metric("–ü—Ä–æ–≥—Ä–∞–º–º—ã üöÄ", "6502")#"{}".format(qprogs))
        col7.write('')

        st.write('')

        data1 = data.groupby('country').count().reset_index().sort_values(by='Link', ascending=False).head(6)
        data2 = data.groupby('format').count().reset_index()

        ce, c1, ce, c2, c3 = st.columns([0.07, 4, 0.07, 4, 0.07])
        with c1:
            fig1 = px.pie(data1, names='country', values='Link', color_discrete_sequence=px.colors.sequential.RdBu, labels={
                        "country": "–°—Ç—Ä–∞–Ω–∞",
                        "n": "–ö–æ–ª–∏—á–µ—Å–≤–æ –ø—Ä–æ–≥—Ä–∞–º–º",
                    }, title='–¢–û–ü-—Å—Ç—Ä–∞–Ω –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–æ–≥—Ä–∞–º–º',  width=600, height=400)
            fig1.update_layout(paper_bgcolor="black",
                                font_color="white")
            st.plotly_chart(fig1, use_container_width=True)

        with c2:
            fig2 = px.pie(data2, names='format', values='Link', color_discrete_sequence=px.colors.diverging.RdYlGn, labels={
                        "format": "–§–æ—Ä–º–∞—Ç",
                        "n": "–ö–æ–ª–∏—á–µ—Å–≤–æ –ø—Ä–æ–≥—Ä–∞–º–º",
                    }, title='–°–æ–æ—Ç–Ω–æ—à—â–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º –ø–æ —Ñ–æ—Ä–º–∞—Ç–∞–º –æ–±—É—á–µ–Ω–∏—è',  width=600, height=400)
            fig2.update_layout(paper_bgcolor="black",
                                font_color="white")
            st.plotly_chart(fig2, use_container_width=True)
        data['duration_month'] = data['duration_month'].astype('float32')
        data3 = data.groupby('country')['tuition_EUR'].agg(['mean']).reset_index().sort_values(by='mean', ascending=True).head(43)
        data4 = data.groupby('country')['duration_month'].agg(['mean']).reset_index().sort_values(by='mean', ascending=False).head(45)
        ce, c1, ce, c2, c3 = st.columns([0.07, 4, 0.07, 4, 0.07])
        with c1:
            fig3 = px.bar(data3, x = "mean", y = "country", orientation='h', labels={
                                "mean": "–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è",
                                "country": "–°—Ç—Ä–∞–Ω–∞"
                            }, title='–°—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø–æ —Å—Ç—Ä–∞–Ω–∞–º')
            fig3.update_traces(marker_color='red', marker_line_color='red',
                            marker_line_width=1, opacity=1)
            fig3.update_layout(legend_font_size=1, width=800,
                height=900, paper_bgcolor="black", font_color='white')
            st.plotly_chart(fig3, use_container_width=True)

        with c2:
            fig6 = px.bar(data4, x = "mean", y = "country", orientation='h', text_auto=True, labels={
                                "mean": "–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–º–µ—Å)",
                                "country": "–°—Ç—Ä–∞–Ω–∞",
                            }, title='–°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è')
            fig6.update_traces(marker_color='red', marker_line_color='red',
                            marker_line_width=1, opacity=1)
            fig6.update_layout(yaxis=dict(autorange="reversed"), legend_font_size=1, width=700,
                height=900, paper_bgcolor="black", font_color='white')
            st.plotly_chart(fig6, use_container_width=True)
        
    st.write('')

    st.write('–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç—Ä–∞–Ω—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –æ–±—ä–µ–º–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞—Ö')
        
    with st.form(key="my_form"):
        country = st.selectbox("–°–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ –Ω–∞—à–µ–π –±–∞–∑–µ —Å—Ç—Ä–∞–Ω", list(set(data['country'].dropna())))
        submit = st.form_submit_button(label="‚ú® –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –≤—ã–±–æ—Ä")
    if submit:
        data_gb = data[data['country']==country]
        with st.expander('{} - –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã–±–æ—Ä!'.format(country)):
            data_f = data_gb.groupby('format').count().reset_index()
            data_l = data_gb.groupby('language').count().reset_index()
            ce, c1, ce, c2, c3 = st.columns([0.07, 4, 0.07, 4, 0.07])
            with c1:
                figf = px.pie(data_f, names='format', values='Link', color_discrete_sequence=px.colors.sequential.RdBu, labels={
                            "format": "–§–æ—Ä–º–∞—Ç",
                            "Link": "–ö–æ–ª–∏—á–µ—Å–≤–æ –ø—Ä–æ–≥—Ä–∞–º–º",
                        }, title='–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º –ø–æ —Ñ–æ—Ä–º–∞—Ç–∞–º –æ–±—É—á–µ–Ω–∏—è',  width=600, height=400)
                figf.update_layout(paper_bgcolor="black",
                                    font_color="white")
                st.plotly_chart(figf, use_container_width=True)

            with c2:
                figl = px.pie(data_l, names='language', values='Link', color_discrete_sequence=px.colors.diverging.RdYlGn, labels={
                            "language": "–Ø–∑—ã–∫ –æ–±—É—á–µ–Ω–∏—è",
                            "Link": "–ö–æ–ª–∏—á–µ—Å–≤–æ –ø—Ä–æ–≥—Ä–∞–º–º",
                        }, title='–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —è–∑—ã–∫–∏ –æ–±—É—á–µ–Ω–∏—è',  width=600, height=400)
                figl.update_layout(paper_bgcolor="black",
                                    font_color="white")
                st.plotly_chart(figl, use_container_width=True)
            
            st.write('')
            data_count = data_gb.groupby('university').count().reset_index().sort_values(by='Link', ascending=False).head(60)
            data_cost = data_gb.groupby('university')['tuition_EUR'].agg(['mean']).reset_index().sort_values(by='mean', ascending=False).head(60)
            c1, c2, c3 = st.columns([0.05, 6 ,0.05])
            with c2:
                fig10 = px.bar(data_count, x = "Link", y = "university", orientation='h', text_auto=True, labels={
                     "Link": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥—Ä–∞–º–º –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ",
                     "university": "–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç",
                 }, title='–í—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞—Ö')

                fig10.update_traces(marker_color='red', marker_line_color='red',
                                marker_line_width=1, opacity=1)

                fig10.update_layout(yaxis=dict(autorange="reversed"), legend_font_size=1, width=1000,
                    height=1100,  paper_bgcolor="black", font_color='white')
                st.plotly_chart(fig10, use_container_width=True)
            
            st.write('')

            c1, c2, c3 = st.columns([0.05, 6 ,0.05])
            with c2:
                fig8 = px.bar(data_cost.dropna(), x = "mean", y = "university", orientation='h', text_auto=True, labels={
                        "mean": "–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏ (EUR/–≥–æ–¥)",
                        "university": "–£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç",
                    }, title='–°—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞—Ö')

                fig8.update_traces(marker_color='red', marker_line_color='red',
                    marker_line_width=1, opacity=1)

                fig8.update_layout(yaxis=dict(autorange="reversed"), legend_font_size=1, width=1000,
                        height=1100,  paper_bgcolor="black", font_color='white')
                st.plotly_chart(fig8, use_container_width=True)
            

                


            
