from gensim.models import Word2Vec
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re 
from collections import defaultdict
import pandas as pd
import folium
from streamlit_folium import folium_static
from folium import plugins
from googletrans import Translator
from PIL import Image
import seaborn as sns

#spreadsheet check
# from gsheetsdb import connect
# from gspread_pandas import Spread,Client
# from google.oauth2 import service_account



st.set_page_config(layout="wide")

#Create a Google Authentication connection object
# scope = ['https://spreadsheets.google.com/feeds',
#          'https://www.googleapis.com/auth/drive']

# credentials = service_account.Credentials.from_service_account_info(
#                 st.secrets["gcp_service_account"], scopes = scope)
# client = Client(scope=scope,creds=credentials)
# spreadsheetname = "Input_holder"
# spread = Spread(spreadsheetname,client = client)


#mean vectorizer
class MeanEmbeddingVectorizer(object):
    def __init__(self, model_cbow):
        self.model_cbow = model_cbow
        self.vector_size = model_cbow.wv.vector_size

    def fit(self):  
        return self

    def transform(self, docs): 
        doc_word_vector = self.doc_average_list(docs)
        return doc_word_vector

    def doc_average(self, doc):
        mean = []
        for word in doc:
            if word in self.model_cbow.wv.index_to_key:
                mean.append(self.model_cbow.wv.get_vector(word))

        if not mean: 
            return np.zeros(self.vector_size)
        else:
            mean = np.array(mean).mean(axis=0)
            return mean

    def doc_average_list(self, docs):
        return np.vstack([self.doc_average(doc) for doc in docs])

#tf-idf vectorized
class TfidfEmbeddingVectorizer(object):
    def __init__(self, model_cbow):

        self.model_cbow = model_cbow
        self.word_idf_weight = None
        self.vector_size = model_cbow.wv.vector_size

    def fit(self, docs): 


        text_docs = []
        for doc in docs:
            text_docs.append(" ".join(doc))

        tfidf = TfidfVectorizer()
        tfidf.fit(text_docs)  
        # if a word was never seen it is given idf of the max of known idf value
        max_idf = max(tfidf.idf_)  
        self.word_idf_weight = defaultdict(
            lambda: max_idf,
            [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()],
        )
        return self

    def transform(self, docs): 
        doc_word_vector = self.doc_average_list(docs)
        return doc_word_vector

    def doc_average(self, doc):


        mean = []
        for word in doc:
            if word in self.model_cbow.wv.index_to_key:
                mean.append(
                    self.model_cbow.wv.get_vector(word) * self.word_idf_weight[word]
                ) 

        if not mean:  
            return np.zeros(self.vector_size)
        else:
            mean = np.array(mean).mean(axis=0)
            return mean
    def doc_average_list(self, docs):
      return np.vstack([self.doc_average(doc) for doc in docs])

@st.cache(allow_output_mutation=True)
def load_data(check): 
    if check: 
        data = pd.read_excel('main_data.xlsx')
        embeddings = pd.read_pickle('embed.pickle')
        clean_words = pd.read_pickle('words.pickle')
        swords = pd.read_pickle('swords.pickle')
        latlong = pd.read_csv('LATandLONG.csv', index_col=0)
        progs = pd.read_pickle('nwconstr.pickle')
    return data, embeddings, clean_words, swords, latlong, progs
data, doc_vec, clean_words, swords, latlong, progs = load_data(True)
data = data[data['tuition_EUR']<90000] #looks shitty, but i don't have ehough time... haha)

# @st.cache(allow_output_mutation=True)
# def corpus_l(data):
#     return list(data)

@st.cache(allow_output_mutation=True)
def load_model(mpath): 
    return Word2Vec.load(mpath)

#load up some cleaning functions
def tokenization(text):
    tokens = re.split('\s+',text)
    return tokens

def remove_stopwords(text):
    output= [i for i in text if i not in swords]
    return output

def len_control(text):
  lemm_text = [word for word in text if len(word)>=3]
  return lemm_text

def sorter(text):
  sorted_list = sorted(text)
  return sorted_list

def make_clickable(name, link):
    # target _blank to open new window
    # extract clickable text to display for your link
    text = name
    return f'<a target="_blank" href="{link}">{text}</a>'

def program_parser2(data):
    for i in range(data.shape[0]):
        data.Introduction[i] = str(re.sub('[0-9]+',' ',re.sub(r'[^\w\s]',' ',re.sub('\\\\n', ' ' ,re.sub('&.*?;.*?;|&.*?;|._....',' ',str(data.Introduction[i]))))).lower().strip())
    data['msg_sorted_clean']= (data['Introduction']
                               .apply(lambda x: tokenization(x))
                               .apply(lambda x:remove_stopwords(x))
                               .apply(lambda x:len_control(x))
                               .apply(lambda x: sorter(x)))
    return data

def pick_n_pretty(df):
    output = df[['Link', 'program', 'university', 'country', 'city ', 'language', 'tuition_EUR','Score']]
    output["Link"] = output.apply(
            lambda row: make_clickable(row["program"], row["Link"]), axis=1)
    output['tuition_EUR'] = output['tuition_EUR'].fillna(0)
    output['tuition_EUR'] = output.apply(lambda row: int(row['tuition_EUR']), axis=1)
    return output#.style.applymap(lambda x: "background-color: red" if x==0 else "background-color: white")


def get_recommendations(N, scores, data_path = 'main_data.xlsx'):
    top = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:N]
    data = (pd.read_excel(data_path, index_col = 0)
           .drop(columns = ['msg_sorted_clean'])
           .loc[top]
           .reset_index())
    data['Score'] = sorted(scores, reverse=True)[:N]
    return data

def get_recs(sentence, N=10, mean=False):
    '''Get top-N recommendations based on your input'''
    input = pd.DataFrame({'Introduction': [str(sentence)]})
    input = program_parser2(input)
    input_embedding = tfidf_vec_tr.transform([input['msg_sorted_clean'][0]])[0].reshape(1, -1)
    cos_sim = map(lambda x: cosine_similarity(input_embedding, x)[0][0], doc_vec)
    scores = list(cos_sim)
    recommendations = get_recommendations(N,scores)
    return recommendations

def mfap(recs1, df=latlong):
    latlong = recs1.merge(df, left_on='city ', right_on='location', how = 'inner')      
    uni_locations = latlong[["lat", "long", "location"]]
    map = folium.Map(width=1000,height=500,location=[uni_locations.lat.mean(), uni_locations.long.mean()], zoom_start=4, control_scale=True)
    for index, location_info in uni_locations.iterrows():
        folium.Marker([location_info["lat"], location_info["long"]], popup=location_info["location"]).add_to(map)
    return map

def mfap_density_50(recs50, df=latlong): #try this function on the main page
    latlong = recs50.merge(df, left_on='city ', right_on='location', how = 'inner')
    uni_locations = latlong[["lat", "long"]]
    map = folium.Map(width=1000,height=500,location=[uni_locations.lat.mean(), uni_locations.long.mean()], zoom_start=4, control_scale=True)
    cityArr = uni_locations.values
    map.add_child(plugins.HeatMap(cityArr, radius=25))
    return map

def sim_prog(df=progs, prog=None):
    df_one = df[df['Program1']==prog]
    return df_one.sort_values(by='cosine', ascending=False)

def p2p_locs(latlong=latlong, uni_info=data, recs=[], N=5): #recs is the output of sim_progs #density map for similar universities
    recs[['Uni', 'Prog']] = recs['Program2'].str.split(': ', 1, expand=True)
    recs[['Uni1', 'Prog1']] = recs['Program1'].str.split(': ', 1, expand=True)
    ps = (recs
            .merge(uni_info, left_on=['Uni', 'Prog'], right_on=['university','program'], how = 'inner')
            .merge(latlong, left_on='city ', right_on='location', how ='inner'))
    fin_rec = ps[['Program1', 'Program2', 'city ','cosine']].reset_index().iloc[1:N+1,:]
    uni_locations = ps[["lat", "long"]]
    map = folium.Map(width=1000,height=500,location=[uni_locations.lat.mean(), uni_locations.long.mean()], zoom_start=4, control_scale=True)
    cityArr = uni_locations.values
    map.add_child(plugins.HeatMap(cityArr, radius=25))
    return map, fin_rec, ps 

def simple_output(map=True):
    col1, col2, col3 = st.columns([10, 10, 10])
    with col2:
        gif_runner = st.image("200.gif")
    recs1 = get_recs(str(text), N=int(number), mean=False)
    recs50 = get_recs(str(text), N=50, mean=False)
    recs1 = pick_n_pretty(recs1)
    gif_runner.empty()
    df = recs1.style.background_gradient(
        cmap=cmGreen,
        subset=[
            "Score",
        ],
    )
    st.write(df.to_html(escape=False), unsafe_allow_html=True)  
    if map:
        map2 = mfap_density_50(recs50) 
        map  = mfap(recs1)
        st.write('')
        st.write('')
        with st.expander('–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –∫–∞—Ä—Ç—ã üåç'):
            A, B = st.columns([5, 5])
            with A:
                st.write('–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤')
                folium_static(map) 
            with B:
                st.write('POI-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–æ–≤ —Ç–æ–ø-50 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –í–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –ø—Ä–æ–≥—Ä–∞–º–º')
                folium_static(map2)


with st.sidebar:
    col1, col2, col3 =st.columns([2.2, 6, 2.2])
    with col1:
        st.write("")
    with col2:
        st.image('keystone-masters-degree.jpg')
    with col3:
        st.write('')
    page = st.radio('–°—Ç—Ä–∞–Ω–∏—Ü–∞', ['–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µüëã',"–ù–∞–π—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º—Éüåç", "–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ãüôå","–ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞üìà"])
    
    # st.subheader('–í—ã–±–µ—Ä–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã')
    # location = st.multiselect('–°—Ç—Ä–∞–Ω–∞', list(set(data['country'])))
    # on_site = st.selectbox('–¢–µ–º–ø –æ–±—É—á–µ–Ω–∏—è', ['–û—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–ó–∞–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ','–û—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ|–ó–∞–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'])
    # pace = st.selectbox('–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è', ['–û–Ω–ª–∞–π–Ω', '–ö–∞–º–ø—É—Å','–ö–∞–º–ø—É—Å|–û–Ω–ª–∞–π–Ω'])
    # lang = st.selectbox('–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è', list(set(data['Language'].dropna())))
    # cost = st.slider('–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, EUR', int(data['tuition_EUR'].min()), int(data['tuition_EUR'].max()), (0, 3000), step=50)

# Page 1-Intro
if page=='–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µüëã':
    img = Image.open("keystone-masters-degree.jpg")
    st.image(img)
  #  st.markdown(dash, unsafe_allow_html = True)
    st.markdown("## How it works? :thought_balloon:")
    #st.write(spread.url)
    st.write(
        "For an in depth overview of the ML methods used and how I created this app, three blog posts are below."
        )
    blog1 = "https://jackmleitch.medium.com/using-beautifulsoup-to-help-make-beautiful-soups-d2670a1d1d52"
    blog2 = "https://towardsdatascience.com/building-a-recipe-recommendation-api-using-scikit-learn-nltk-docker-flask-and-heroku-bfc6c4bdd2d4"
    blog3 = "https://towardsdatascience.com/building-a-recipe-recommendation-system-297c229dda7b"
    st.markdown(
        f"1. [Web Scraping Cooking Data With Beautiful Soup]({blog1})"
        )
    st.markdown(
            f"2. [Building a Recipe Recommendation API using Scikit-Learn, NLTK, Docker, Flask, and Heroku]({blog2})"
        )
    st.markdown(
            f"3. [Building a Recipe Recommendation System Using Word2Vec, Scikit-Learn, and Streamlit]({blog3})"
        )
    #st.write(spread.url)

   # st.markdown(hello, unsafe_allow_html = True)

if page=='–ù–∞–π—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º—Éüåç':
    #_max_width_()
    c30, c31, c32 = st.columns([2.5, 1, 3])

    with c30:
        st.image("keystone-masters-degree.jpg", width=400)

    with st.expander("‚ÑπÔ∏è - –û–± —ç—Ç–æ–π —Å—Ç–∞—Ä–Ω–∏—Ü–µ", expanded=False):

        st.write(
            """
            –ù–∞–¥–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã - —Ö—É–µ—Ç–∞ –∫—Ä—á     
    -   The *BERT Keyword Extractor* app is an easy-to-use interface built in Streamlit for the amazing [KeyBERT](https://github.com/MaartenGr/KeyBERT) library from Maarten Grootendorst!
    -   It uses a minimal keyword extraction technique that leverages multiple NLP embeddings and relies on [Transformers] (https://huggingface.co/transformers/) ü§ó to create keywords/keyphrases that are most similar to a document.
            """
        )

        st.markdown("")

    st.markdown("")

    #scenario_interact = st.selectbox(
     #   "–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –æ–ø—ã—Ç–∞",
      #  ["–•–æ—á—É –Ω–∞–π—Ç–∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–•–æ—á—É —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å–≤–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≥—Ä–∞–º–º"],
    

    with st.form(key="my_form"):

        ce, c1, ce, c2, c3 = st.columns([0.07, 2, 0.07, 4, 0.07])
        with c1:
            st.subheader('–í—ã–±–µ—Ä–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã') 
            number = st.number_input('–°–∫–æ–ª—å–∫–æ —Ä–µ–∫–æ–º–º–µ–Ω–¥–∞—Ü–∏–π –∂–µ–ª–∞–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å –Ω–∞ —ç–∫—Ä–∞–Ω–µ?', min_value=0, max_value=50, step=1, value=5)
            agree = st.checkbox('–í—ã–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é')
            location = st.multiselect('–°—Ç—Ä–∞–Ω–∞', sorted(list(set(data['country'].dropna()))))
            on_site = st.selectbox('–¢–µ–º–ø –æ–±—É—á–µ–Ω–∏—è', ['–û—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–ó–∞–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ','–û—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ|–ó–∞–æ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'])
            pace = st.selectbox('–§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è', ['–û–Ω–ª–∞–π–Ω', '–ö–∞–º–ø—É—Å','–ö–∞–º–ø—É—Å|–û–Ω–ª–∞–π–Ω'])
            lang = st.multiselect('–Ø–∑—ã–∫ –æ–±—É—á–µ–Ω–∏—è', sorted(list(set(data['language'].dropna()))))
            cost = st.slider('–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, EUR', int(data['tuition_EUR'].min()), int(data['tuition_EUR'].max()), (0, 8000), step=50)
        with c2:
            st.write('''
            
            
            
            ''') #to make row effects
            st.markdown('')
            st.markdown('')
            sentence = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Å–≤–æ–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π -- –º–æ–∂–µ—Ç–µ –≤–≤–µ—Å—Ç–∏ —á—Ç–æ —É–≥–æ–¥–Ω–æ, –Ω–æ —Ü–∏—Ñ—Ä—ã –∏ —Å–∏–º–≤–æ–ª—ã –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º–æ–π", value='–ù–∞–ø—Ä–∏–º–µ—Ä: —è –∑–Ω–∞—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É, –ø—Ä–æ—à–µ–ª –∫—É—Ä—Å—ã –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Å—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ —Ä—ã–Ω–∫–∞–º–∏')
            submit = st.form_submit_button(label="‚ú® –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é")
            corpus = list(clean_words)
            model = load_model('model_cbow.bin')
            model.init_sims(replace=True)
            tfidf_vec_tr = TfidfEmbeddingVectorizer(model)
            tfidf_vec_tr.fit(corpus)
            translator = Translator()
            result = translator.translate(sentence)
            text = result.text

    if not submit:
        st.stop()

    cmGreen = sns.light_palette("green", as_cmap=True)
    if submit:
        if len(text)==0:
            st.warning('–í—ã –Ω–µ —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏ –æ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö! –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —Å–∏—Å—Ç–µ–º–∞ –≤—ã–¥–∞—Å—Ç –ø–µ—Ä–≤—ã–µ {} —Å—Ç—Ä–æ–∫(–∏) –Ω–∞—à–µ–π –±–∞–∑—ã —Å –ø—Ä–æ–≥—Ä–∞–º–º–∞–º–∏.... –≠—Ç–æ –Ω–µ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ'.format(number))
            simple_output()
        else:
            if not agree:
                if len(location)>0 and len(lang)>0:  
                    col1, col2, col3 = st.columns([10, 10, 10])
                    with col1:
                        st.write('')
                    with col2:
                        gif_runner = st.image("200.gif")
                    with col3:
                        st.write('')
                    recs = get_recs(str(text), N=int(number), mean=False)
                    recs50 = get_recs(str(text), N=50, mean=False)
                    gif_runner.empty()  
                    recs1 = recs[(recs['language'].isin(list(lang))) & (recs['country'].isin(list(location))) & (recs['on_site']==on_site) & (recs['format']==pace) & (recs['tuition_EUR']>min(cost)) & (recs['tuition_EUR']<max(cost))]
                    if recs1.shape[0]!=0:
                        recs2 = pick_n_pretty(recs1)
                        df = recs2.style.background_gradient(
                            cmap=cmGreen,
                            subset=[
                                "Score",
                            ],
                        )
                        st.write(df.to_html(escape=False), unsafe_allow_html=True)
                        map2 = mfap_density_50(recs50)
                        map  = mfap(recs2)
                        st.write('')
                        st.write('')
                        with st.expander('–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –∫–∞—Ä—Ç—ãüåç'):
                            A, B = st.columns([5, 5])
                            with A:
                                st.write('–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤')
                                folium_static(map) 
                            with B:
                                st.write('POI-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–æ–≤ —Ç–æ–ø-50 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –í–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –ø—Ä–æ–≥—Ä–∞–º–º')
                                folium_static(map2)
                        if recs1.shape[0]<number:
                            st.warning("–£–ø—Å... –ü—Ä–æ–≥—Ä–∞–º–º –º–µ–Ω—å—à–µ —á–µ–º –æ–∂–∏–¥–∞–ª–æ—Å—å, –Ω–æ —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å... –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –æ–ø—Ü–∏—é –Ω–∏–∂–µ)")
                            recs1 = recs.copy()
                            recs2 = pick_n_pretty(recs1)
                            map3 = mfap(recs2)
                            df = recs2.style.background_gradient(
                                cmap=cmGreen,
                                subset=[
                                    "Score",
                                ],
                            )
                            with st.expander('–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–ø—Ü–∏—è–º–∏ –∏–∑ –Ω–∞—à–µ–π –±–∞–∑—ã üëâ'):
                                st.write(df.to_html(escape=False), unsafe_allow_html=True)
                                C, D, E = st.columns([2,5,2])
                                with C:
                                    st.write('')
                                with D:
                                    st.write('–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –≤—ã—à–µ —Ç–∞–±–ª–∏—Ü—ã')
                                    folium_static(map3)
                                with E:
                                    st.write('')

                            
                    else:
                        st.warning('–ú—ã –Ω–µ —Å–º–æ–≥–ª–∏ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –í–∞—à–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º, –Ω–æ –ø—Ä–æ—Å–∏–º –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –≤ –Ω–∞—à–µ–π –±–∞–∑–µ ')
                        simple_output()
                else: 
                    st.write('This is an error') #–ù–∞–¥–æ –±—É–¥–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–æ–ø–∏—Å–∞—Ç—å

            else: 
                simple_output()
if page=='–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ãüôå':
    c30, c31, c32 = st.columns([2.5, 1, 3])

    with c30:
        st.image("keystone-masters-degree.jpg", width=400)

    with st.expander("‚ÑπÔ∏è - –û–± —ç—Ç–æ–π —Å—Ç–∞—Ä–Ω–∏—Ü–µ", expanded=False):

        st.write(
            """
            –ù–∞–¥–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã - —Ö—É–µ—Ç–∞ –∫—Ä—á     
    -   The *BERT Keyword Extractor* app is an easy-to-use interface built in Streamlit for the amazing [KeyBERT](https://github.com/MaartenGr/KeyBERT) library from Maarten Grootendorst!
    -   It uses a minimal keyword extraction technique that leverages multiple NLP embeddings and relies on [Transformers] (https://huggingface.co/transformers/) ü§ó to create keywords/keyphrases that are most similar to a document.
            """
        )

        st.markdown("")

    st.markdown("")    

    st.write('–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω—É –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞')
    with st.form(key="my_form"):
        university_pick = st.selectbox("–°–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤ –Ω–∞—à–µ–π –±–∞–∑–µ –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º", list(set(progs['Program1'].dropna())))
        number_sim = st.number_input('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º', min_value=0, max_value=50, step=1, value=5)
        submit = st.form_submit_button(label="‚ú® –ü–æ–∫–∞–∑–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã")
        cmGreen = sns.light_palette("green", as_cmap=True)
    if submit:
        recs = sim_prog(progs, str(university_pick))
        map, recs0, ps = p2p_locs(recs=recs, N=number_sim) #ps is a dirty dataset
        df = recs0.sort_values(by='cosine', ascending=False).style.background_gradient(
            cmap=cmGreen,
            subset=[
                "cosine",
            ],
        )
        see_data = st.expander('–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ö–æ–∂–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã üëâ')
        with see_data:
            st.write(df.to_html(escape=False), unsafe_allow_html=True)
        st.write('')
        a, b = st.columns([5,5])
        with a:
            st.write('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º')
            folium_static(map)
        with b:
            st.write('')
            c_metric = ps.groupby('Program1')['Program2'].count()[0]-1
            st.metric(label='–°—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º', value='{}'.format(c_metric))
            d_metric = ps[ps.Uni==ps.Uni1].shape[0]-1
            st.metric(label='–í –æ–¥–Ω–æ–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ', value='{}'.format(d_metric))
            top = ps.groupby(['city '])['Program2'].agg(['count']).sort_values(by = 'count', ascending=False).head()
            df = pd.DataFrame({'–ì–æ—Ä–æ–¥':list(top.index), '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': list(top['count'])})
            st.write('–í—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç—å —Å—Ç—Ä–∞–Ω —Å—Ö–æ–∂–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º')
            st.table(df)


   
if page == '–ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞üìà':
    st.title('–ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞')